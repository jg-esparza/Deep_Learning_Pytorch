{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:46.838295Z",
     "start_time": "2025-01-29T19:41:40.331775Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.113332Z",
     "start_time": "2025-01-29T19:41:46.840827Z"
    }
   },
   "source": [
    "mnist_train = pd.read_csv('../datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('../datasets/mnist-in-csv/mnist_test.csv')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd /content/drive/My Drive/Colab Notebooks/\n",
    "# mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "# mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.140521Z",
     "start_time": "2025-01-29T19:41:49.114352Z"
    }
   },
   "source": [
    "mnist_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.310394Z",
     "start_time": "2025-01-29T19:41:49.142025Z"
    }
   },
   "source": [
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample\n",
    "* We will use transpose to change the shape of image tensor<br>\n",
    "<b>.imshow()</b> needs a 2D array, or a 3D array with the third dimension being of size 3 or 4 only (For RGB or RGBA), so we will shift first axis to last<br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.321959Z",
     "start_time": "2025-01-29T19:41:49.311935Z"
    }
   },
   "source": [
    "random_sel = mnist_train.sample(8) # select randomly 8 images\n",
    "\n",
    "random_sel.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.368239Z",
     "start_time": "2025-01-29T19:41:49.324473Z"
    }
   },
   "source": [
    "image_features = random_sel.drop('label', axis = 1)\n",
    "\n",
    "image_batch = (torch.Tensor(image_features.values / 255.)).reshape((-1, 28, 28)) # normalization\n",
    "\n",
    "image_batch.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.402957Z",
     "start_time": "2025-01-29T19:41:49.369245Z"
    }
   },
   "source": [
    "grid = torchvision.utils.make_grid(image_batch.unsqueeze(1), nrow=8)\n",
    "\n",
    "grid.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 242])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.511120Z",
     "start_time": "2025-01-29T19:41:49.403966Z"
    }
   },
   "source": [
    "plt.figure (figsize = (12, 12))\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "plt.axis('off')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 241.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying features and labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.651678Z",
     "start_time": "2025-01-29T19:41:49.511631Z"
    }
   },
   "source": [
    "mnist_train_features = mnist_train.drop('label', axis =1)\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "mnist_test_features = mnist_test.drop('label', axis =1)\n",
    "mnist_test_target = mnist_test['label']"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.715743Z",
     "start_time": "2025-01-29T19:41:49.652272Z"
    }
   },
   "source": [
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "x_test_tensor  = torch.tensor(mnist_test_features.values, dtype=torch.float) \n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.722254Z",
     "start_time": "2025-01-29T19:41:49.716751Z"
    }
   },
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 784])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping the tensors according to what the CNN needs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.729290Z",
     "start_time": "2025-01-29T19:41:49.723338Z"
    }
   },
   "source": [
    "# first dimension is the batch,-1 means that it depends on whatever values we set\n",
    "# second dimension is channel (grayscale)\n",
    "# third and fourth dimensions are height and width\n",
    "\n",
    "X_train_tensor = X_train_tensor.reshape(-1, 1, 28, 28) \n",
    "\n",
    "x_test_tensor = x_test_tensor.reshape(-1, 1, 28, 28)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.738530Z",
     "start_time": "2025-01-29T19:41:49.730294Z"
    }
   },
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining  CNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.747295Z",
     "start_time": "2025-01-29T19:41:49.739918Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the neural network\n",
    "* The input size will be the channels of the images (in_size)\n",
    "* The final output will have a size equal to the number of classes for the prediction\n",
    "* The convolving kernel will have a size of k_conv_size"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.755307Z",
     "start_time": "2025-01-29T19:41:49.748302Z"
    }
   },
   "source": [
    "# Input size is the number of channels in the input image, for grayscale images this will be 1\n",
    "in_size = 1\n",
    "\n",
    "# first convolutional layer has the size of 16, and so on\n",
    "hid1_size = 16 #Re-run for 32\n",
    "hid2_size = 32 #Re-run for 64\n",
    "\n",
    "# final layer, 10 is the total classes\n",
    "out_size = 10\n",
    "\n",
    "k_conv_size = 5 #re-run for 3"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Convolutional Neural Network\n",
    "\n",
    "<b>Conv2d: </b>Applies a 2D convolution over an input signal composed of several input planes.<br>\n",
    "Parameters<br>\n",
    "in_channels (int) – Number of channels in the input image<br>\n",
    "out_channels (int) – Number of channels produced by the convolution<br>\n",
    "kernel_size (int or tuple) – Size of the convolving kernel<br>\n",
    "\n",
    "<b>BatchNorm2d: </b>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "Parameters<br>\n",
    "num_features – C from an expected input of size (N,C,H,W)\n",
    "\n",
    "<b>ReLU: </b>Activation function\n",
    "\n",
    "<b>Maxpool2d: </b>\n",
    "Parameters:<br>\n",
    "kernel_size – the size of the window to take a max over\n",
    "\n",
    "<b>Linear: </b>\n",
    "Parameter:<br>\n",
    "\n",
    "in_features: \n",
    "All the operations above used 4D Tensors of shape \n",
    "\n",
    "Now for fully connected layers(linear layers) we need to transform them in 1D Tensors<br>\n",
    "So to the in_features of fully connected layer we will give size\n",
    "out_features:<br>\n",
    "num_classes = number of output labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.764419Z",
     "start_time": "2025-01-29T19:41:49.756310Z"
    }
   },
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.fc = nn.Linear(512, out_size)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        ## F.log_softmax(out, dim=-1)\n",
    "        \n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.790819Z",
     "start_time": "2025-01-29T19:41:49.765927Z"
    }
   },
   "source": [
    "model = ConvNet()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:50.873078Z",
     "start_time": "2025-01-29T19:41:49.794328Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.019306Z",
     "start_time": "2025-01-29T19:41:50.873600Z"
    }
   },
   "source": [
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.066353Z",
     "start_time": "2025-01-29T19:41:51.020314Z"
    }
   },
   "source": [
    "X_train_tensor = X_train_tensor.to(device)\n",
    "x_test_tensor  = x_test_tensor.to(device) \n",
    "\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "y_test_tensor  = y_test_tensor.to(device)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.071482Z",
     "start_time": "2025-01-29T19:41:51.067379Z"
    }
   },
   "source": [
    "#Re-run for each different value\n",
    "\n",
    "learning_rate = 0.001 \n",
    "#0.01 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#nn.NLLLoss() \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#optimizer =torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "                    "
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:42:33.659035Z",
     "start_time": "2025-01-29T19:41:51.072005Z"
    }
   },
   "source": [
    "num_epochs = 10\n",
    "loss_values = list()\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "        \n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs,Y_train_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    \n",
    "        print('Epoch - %d, loss - %0.5f '%(epoch, loss.item()))\n",
    "        loss_values.append(loss.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 12, 12])\n",
      "torch.Size([60000, 32, 4, 4])\n",
      "torch.Size([60000, 512])\n",
      "torch.Size([60000, 10])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.40 GiB is allocated by PyTorch, and 4.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs,Y_train_tensor)\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 10\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep() \n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch - \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, loss - \u001B[39m\u001B[38;5;132;01m%0.5f\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m%\u001B[39m(epoch, loss\u001B[38;5;241m.\u001B[39mitem()))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.40 GiB is allocated by PyTorch, and 4.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = (range(0, 9))\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y_test = y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(predicted, y_test))\n",
    "    print(\"Precision: \", precision_score(predicted, y_test, average='weighted'))\n",
    "    print(\"Recall: \", recall_score(predicted, y_test, average='weighted'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model for predictions "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"sample target data = \", mnist_test_target.values[1005])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_img = mnist_test_features.values[1005]\n",
    "sample_img = sample_img.reshape(1, 28, 28)\n",
    "\n",
    "sample_img = sample_img[0, :, :]\n",
    "\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.imshow(sample_img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample = np.array(mnist_test_features.values[1005]) \n",
    "\n",
    "sample_tensor = torch.from_numpy(sample).float()\n",
    "sample_tensor = sample_tensor.reshape(-1, 1, 28, 28)\n",
    "sample_tensor = sample_tensor.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_pred = model(sample_tensor)\n",
    "y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, predicted = torch.max(y_pred.data, -1)\n",
    "\n",
    "print (\" The predicted label is : \", predicted.item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.device_count()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.current_device()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.device(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.get_device_name(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:43:35.372863Z",
     "start_time": "2025-01-29T19:43:35.366250Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 4.3 GB\n",
      "Cached:    10.5 GB\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
